<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>AI Daily 2026-02-12</title>
  <meta name="description" content="AI Daily 2026-02-12 论文精选" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&family=IBM+Plex+Serif:wght@400;600;700&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet" />
  <style>
    :root {
      --bg: #f7f5f1;
      --surface: #fffdf8;
      --ink: #171717;
      --muted: #55514a;
      --line: #d9d2c7;
      --accent: #9e4b23;
      --accent-soft: #f2e4d8;
      --radius: 14px;
      --shadow: 0 8px 30px rgba(25, 20, 12, 0.06);
      --reading: 72ch;
    }

    * { box-sizing: border-box; }

    body {
      margin: 0;
      color: var(--ink);
      font-family: "IBM Plex Sans", "PingFang SC", "Noto Sans CJK SC", sans-serif;
      line-height: 1.78;
      background:
        radial-gradient(circle at 12% 8%, #efe7db, transparent 35%),
        radial-gradient(circle at 84% 14%, #ece5db, transparent 38%),
        var(--bg);
    }

    .page {
      max-width: 1180px;
      margin: 0 auto;
      padding: 34px 18px 72px;
    }

    .masthead {
      border-top: 3px solid var(--accent);
      padding-top: 16px;
      margin-bottom: 30px;
    }

    .eyebrow {
      margin: 0 0 10px;
      font-family: "IBM Plex Mono", monospace;
      color: var(--accent);
      letter-spacing: 0.08em;
      text-transform: uppercase;
      font-size: 0.8rem;
    }

    h1 {
      margin: 0;
      font-family: "IBM Plex Serif", "Noto Serif CJK SC", serif;
      font-size: clamp(2rem, 4.3vw, 3.45rem);
      line-height: 1.1;
      letter-spacing: -0.01em;
      max-width: 18ch;
    }

    .masthead-meta {
      margin-top: 16px;
      display: flex;
      gap: 14px;
      flex-wrap: wrap;
      color: var(--muted);
      font-family: "IBM Plex Mono", monospace;
      font-size: 0.86rem;
    }

    .layout {
      display: grid;
      grid-template-columns: minmax(0, 1fr) 320px;
      gap: 26px;
      align-items: start;
    }

    .section-title {
      margin: 0 0 12px;
      font-family: "IBM Plex Serif", "Noto Serif CJK SC", serif;
      font-size: clamp(1.35rem, 2vw, 1.72rem);
      line-height: 1.25;
      letter-spacing: -0.01em;
    }

    .section-title::after {
      content: "";
      display: block;
      width: 70px;
      height: 2px;
      margin-top: 7px;
      background: var(--accent);
    }

    .paper-list {
      max-width: 100%;
    }

    .paper-card {
      background: var(--surface);
      border: 1px solid var(--line);
      border-radius: var(--radius);
      padding: 22px 24px;
      margin-bottom: 16px;
      box-shadow: var(--shadow);
      opacity: 0;
      transform: translateY(10px);
      animation: fadeIn 460ms ease forwards;
    }

    .paper-header {
      margin-bottom: 6px;
    }

    .paper-rank {
      color: var(--accent);
      font-family: "IBM Plex Mono", monospace;
      font-size: 0.82rem;
      margin-bottom: 6px;
      letter-spacing: 0.03em;
      text-transform: uppercase;
    }

    .paper-title {
      margin: 0;
      font-family: "IBM Plex Serif", "Noto Serif CJK SC", serif;
      font-size: clamp(1.24rem, 2.2vw, 1.58rem);
      line-height: 1.28;
      max-width: var(--reading);
    }

    .paper-authors {
      margin: 8px 0;
      color: var(--muted);
      font-size: 0.94rem;
    }

    .paper-brief {
      margin: 0 0 10px;
      max-width: var(--reading);
      font-size: 1rem;
    }

    .paper-insights {
      margin: 0 0 12px;
      padding-left: 1.15rem;
      max-width: var(--reading);
    }

    .paper-insights li {
      margin-bottom: 6px;
    }

    .tags {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-bottom: 8px;
    }

    .tag {
      font-family: "IBM Plex Mono", monospace;
      font-size: 0.74rem;
      border: 1px solid var(--line);
      background: #fcf7ef;
      color: #6b3f27;
      border-radius: 999px;
      padding: 2px 10px;
    }

    a {
      color: #8f3a18;
      text-decoration-color: rgba(159, 77, 37, 0.48);
      text-underline-offset: 2px;
      transition: color 180ms ease, text-decoration-color 180ms ease;
    }

    a:hover {
      color: #6b280f;
      text-decoration-color: rgba(107, 40, 15, 0.78);
    }

    .paper-link {
      margin: 0;
      font-weight: 500;
    }

    .sidebar {
      position: sticky;
      top: 18px;
      display: grid;
      gap: 14px;
    }

    .focus-card,
    .takeaway {
      margin: 0;
      background: var(--surface);
      border: 1px solid var(--line);
      border-radius: var(--radius);
      padding: 16px 18px;
      box-shadow: var(--shadow);
    }

    .focus-title {
      margin: 0 0 8px;
      font-family: "IBM Plex Serif", "Noto Serif CJK SC", serif;
      font-size: 1.12rem;
      line-height: 1.3;
    }

    .focus-card p,
    .takeaway p,
    .focus-card li,
    .takeaway li {
      margin: 0 0 6px;
      color: #2a2825;
      font-size: 0.95rem;
    }

    .focus-card ul,
    .takeaway ul {
      margin: 0;
      padding-left: 1.1rem;
    }

    blockquote {
      margin: 12px 0;
      padding: 8px 14px;
      border-left: 3px solid var(--accent);
      background: var(--accent-soft);
      border-radius: 6px;
      color: #3a2f27;
      font-style: italic;
    }

    pre,
    code {
      font-family: "IBM Plex Mono", ui-monospace, SFMono-Regular, Menlo, monospace;
    }

    pre {
      margin: 12px 0;
      padding: 12px;
      overflow-x: auto;
      background: #f2ede5;
      border: 1px solid #ded3c5;
      border-radius: 8px;
      line-height: 1.5;
    }

    code {
      background: #f3ede3;
      border: 1px solid #e4d8c8;
      border-radius: 4px;
      padding: 0 4px;
      font-size: 0.92em;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 12px 0;
      font-size: 0.94rem;
    }

    th,
    td {
      border-bottom: 1px solid var(--line);
      padding: 7px 8px;
      text-align: left;
      vertical-align: top;
    }

    th {
      font-family: "IBM Plex Serif", "Noto Serif CJK SC", serif;
      font-weight: 600;
    }

    footer {
      margin-top: 34px;
      padding-top: 16px;
      border-top: 1px solid var(--line);
      display: flex;
      justify-content: space-between;
      flex-wrap: wrap;
      gap: 10px;
      color: var(--muted);
      font-size: 0.9rem;
    }

    @keyframes fadeIn {
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    @media (max-width: 980px) {
      .layout {
        grid-template-columns: 1fr;
      }

      .sidebar {
        position: static;
      }
    }

    @media (max-width: 640px) {
      .page {
        padding: 24px 14px 52px;
      }

      .paper-card {
        padding: 18px 16px;
      }
    }
  </style>
</head>
<body>
  <main class="page">
    <header class="masthead">
      <p class="eyebrow">AI Daily / Longform Digest</p>
      <h1>Frontier AI Papers, Curated as an Editorial Daily Brief</h1>
      <div class="masthead-meta">
        <span>DATE 2026-02-12</span>
        <span>PAPERS 10</span>
      </div>
    </header>

    <div class="layout">
      <section class="paper-list">
        <h2 class="section-title">Top 10 Papers</h2>
        <article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 01</div><h3 class="paper-title">LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs</h3></header><p class="paper-authors">Benno Krojer, Shravan Nayak, Oscar Mañas, Vaibhav Adlakha, Desmond Elliott, Siva Reddy, Marius Mosbach</p><p class="paper-brief">LatentLens 通过将视觉 token 的潜在表示与上下文化文本表示进行最近邻匹配，揭示了 VLM 中大多数视觉 token 在所有层都是可解释的。</p><ul class="paper-insights"><li>问题：LatentLens 通过将视觉 token 的潜在表示与上下文化文本表示进行最近邻匹配，揭示了 VLM 中大多数视觉 token 在所有层都是可解释的。</li><li>方法：LatentLens 方法：将潜在表示映射到自然语言描述的新方法；上下文化文本表示匹配：预先编码大量文本语料，存储上下文化 token 表示，用 KNN 匹配视觉 token</li><li>结果：在 10 个 VLM 上，大多数视觉 token 在所有层都是可解释的；常用方法 LogitLens 显著低估了视觉 token 的可解释性</li><li>意义：视觉 token 在 LLM 中的可解释性被严重低估；上下文化表示比 logit 空间投影更能反映视觉 token 的语义</li></ul><div class="tags"><span class="tag">Interpretability</span><span class="tag">VLM</span><span class="tag">Visual Tokens</span><span class="tag">Representation Analysis</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.00462" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 02</div><h3 class="paper-title">ASA: Training-Free Representation Engineering for Tool-Calling Agents</h3></header><p class="paper-authors">Youjin Wang, Run Zhou, Rong Fu, Shuaishuai Cao, Hongwei Zeng, Jiaxuan Lu, Sicheng Fan, Jiaqiao Zhao, Liangming Pan</p><p class="paper-brief">ASA 发现 LLM 中间层已能完美解码工具使用意图但行为保守（&quot;懒惰代理&quot;问题），通过推理时单次中间层激活干预，仅需约 20KB 资产即可大幅提升工具调用 F1。</p><ul class="paper-insights"><li>问题：ASA 发现 LLM 中间层已能完美解码工具使用意图但行为保守（&quot;懒惰代理&quot;问题），通过推理时单次中间层激活干预，仅需约 20KB 资产即可大幅提升工具调用 F1。</li><li>方法：懒惰代理（Lazy Agent）失败模式：发现工具必要性在中间层激活中几乎完美可解码，但模型在进入工具模式时仍然保守；激活引导适配器（ASA）：无需训练的推理时控制器，执行单次中间层干预</li><li>结果：严格工具使用 F1 从 0.18 提升到 0.50；误报率从 0.15 降至 0.05</li><li>意义：&quot;懒惰代理&quot;问题是工具调用失败的关键原因；表示工程方法可以免训练地解决行为问题</li></ul><div class="tags"><span class="tag">Agent</span><span class="tag">Tool-Calling</span><span class="tag">Representation Engineering</span><span class="tag">Training-Free</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.04935" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 03</div><h3 class="paper-title">G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design</h3></header><p class="paper-authors">Baoyun Zhao, He Wang, Liang Zeng</p><p class="paper-brief">G-LNS 提出了一个生成式进化框架，利用 LLM 协同进化 LNS 的破坏-修复算子对，在 TSP 和 CVRP 上显著超越现有 LLM AHD 方法和经典求解器。</p><ul class="paper-insights"><li>问题：G-LNS 提出了一个生成式进化框架，利用 LLM 协同进化 LNS 的破坏-修复算子对，在 TSP 和 CVRP 上显著超越现有 LLM AHD 方法和经典求解器。</li><li>方法：LNS 算子自动设计：将 LLM 的自动启发式设计扩展到大邻域搜索算子；破坏-修复算子协同进化：LLM 同时进化紧密耦合的破坏和修复算子对</li><li>结果：在 TSP 和 CVRP 基准上显著超越 LLM AHD 方法；超越强经典求解器</li><li>意义：LLM 不仅能写代码，还能设计组合优化算法；协同进化耦合算子比单独进化更有效</li></ul><div class="tags"><span class="tag">LLM</span><span class="tag">Combinatorial Optimization</span><span class="tag">Heuristic Design</span><span class="tag">LNS</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.08253" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 04</div><h3 class="paper-title">TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions</h3></header><p class="paper-authors">Linli Yao, Yuancheng Wei, Yaojie Zhang, Lei Li, Xinlong Chen, Feifan Song, Ziyue Wang, Kun Ouyang, Yuanxin Liu, Lingpeng Kong</p><p class="paper-brief">提出&quot;全方位密集字幕&quot;任务和 TimeChat-Captioner-7B 模型，通过六维结构化描述生成电影剧本式的视频叙事，超越 Gemini-2.5-Pro。</p><ul class="paper-insights"><li>问题：提出&quot;全方位密集字幕&quot;任务和 TimeChat-Captioner-7B 模型，通过六维结构化描述生成电影剧本式的视频叙事，超越 Gemini-2.5-Pro。</li><li>方法：全方位密集字幕（Omni Dense Captioning）任务：生成带时间戳的连续、细粒度、结构化音视觉叙事；六维结构化模式：创建&quot;剧本式&quot;字幕，让读者能逐场景想象视频内容</li><li>结果：超越 Gemini-2.5-Pro 的密集字幕质量；生成的密集描述显著提升下游音视觉推理（DailyOmni 和 WorldSense）</li><li>意义：视频理解从简单描述走向结构化剧本式叙事；GRPO 在视频字幕任务中被成功应用</li></ul><div class="tags"><span class="tag">Video Understanding</span><span class="tag">Dense Captioning</span><span class="tag">GRPO</span><span class="tag">Multimodal</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.08711" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 05</div><h3 class="paper-title">iGRPO: Self-Feedback-Driven LLM Reasoning</h3></header><p class="paper-authors">Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz</p><p class="paper-brief">iGRPO 是 GRPO 的两阶段迭代扩展，通过模型生成的草稿进行动态自条件化，在 AIME24/25 上达到新 SOTA（85.62%/79.64%）。</p><ul class="paper-insights"><li>问题：iGRPO 是 GRPO 的两阶段迭代扩展，通过模型生成的草稿进行动态自条件化，在 AIME24/25 上达到新 SOTA（85.62%/79.64%）。</li><li>方法：两阶段迭代 GRPO：在标准 GRPO 基础上增加自条件化精炼阶段；动态自条件化：Stage 1 采样多个探索性草稿并选择最佳；Stage 2 将最佳草稿附加到提示中进行精炼</li><li>结果：AIME24: 85.62% (新 SOTA)；AIME25: 79.64% (新 SOTA)</li><li>意义：自反馈是提升 RL 推理训练效果的有效策略；&quot;先探索后精炼&quot;范式优于直接优化</li></ul><div class="tags"><span class="tag">RL</span><span class="tag">GRPO</span><span class="tag">Reasoning</span><span class="tag">Math</span><span class="tag">Self-Improvement</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.09000" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 06</div><h3 class="paper-title">Towards Autonomous Mathematics Research</h3></header><p class="paper-authors">Tony Feng, Trieu H. Trinh, Garrett Bingham, Dawsen Hwang, Yuri Chervonyi, Junehyuk Jung, Joonkyung Lee 等 (28 位作者)</p><p class="paper-brief">Google DeepMind 推出数学研究代理 Aletheia，基于 Gemini Deep Think 实现从竞赛级解题到专业数学研究的跨越，包括自主解决 4 个开放问题。</p><ul class="paper-insights"><li>问题：Google DeepMind 推出数学研究代理 Aletheia，基于 Gemini Deep Think 实现从竞赛级解题到专业数学研究的跨越，包括自主解决 4 个开放问题。</li><li>方法：数学研究代理 Aletheia：迭代地生成、验证和修订端到端自然语言数学解决方案；超越竞赛的推理时扩展定律：将推理时 scaling 从竞赛问题扩展到研究级别</li><li>结果：AI 独立研究论文：Feng26——AI 无人干预计算算术几何中的结构常数；人机协作：LeeSeo26——证明交互粒子系统（独立集）的界</li><li>意义：AI 数学研究从竞赛解题正式迈入专业研究领域；推理时 scaling 在开放研究问题上同样有效</li></ul><div class="tags"><span class="tag">Math Research</span><span class="tag">Agent</span><span class="tag">Autonomous Research</span><span class="tag">Gemini</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.10177" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 07</div><h3 class="paper-title">When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning</h3></header><p class="paper-authors">Leheng Sheng, Yongtao Zhang, Wenchang Ma, Yaorui Shi, Ting Huang, Xiang Wang, An Zhang, Ke Shen, Tat-Seng Chua</p><p class="paper-brief">GRU-Mem 通过引入更新门和退出门，解决了 MemAgent 在长上下文推理中记忆爆炸和不必要计算的问题，实现最高 400% 的推理加速。</p><ul class="paper-insights"><li>问题：GRU-Mem 通过引入更新门和退出门，解决了 MemAgent 在长上下文推理中记忆爆炸和不必要计算的问题，实现最高 400% 的推理加速。</li><li>方法：文本控制的双门机制：更新门决定何时更新记忆，退出门决定何时停止循环；端到端 RL 奖励设计：引入 r^{update} 和 r^{exit} 两个奖励信号，训练模型学会正确的更新和退出行为</li><li>结果：在多种长上下文推理任务上普遍优于原始 MemAgent；最高实现 400% 推理速度加速</li><li>意义：长上下文推理的关键不仅是&quot;记什么&quot;，还有&quot;何时记&quot;和&quot;何时停&quot;；GRU 风格的门控机制在文本记忆管理中非常有效</li></ul><div class="tags"><span class="tag">Long-Context</span><span class="tag">Memory</span><span class="tag">RL</span><span class="tag">Reasoning</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.10560" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 08</div><h3 class="paper-title">Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters</h3></header><p class="paper-authors">Ailin Huang, Ang Li, Aobo Kong, Bin Wang, Binxing Jiao 等 (200+ 位作者)</p><p class="paper-brief">Step 3.5 Flash 是一个 196B 参数、11B 活跃参数的稀疏 MoE 模型，通过可扩展的强化学习框架在数学、代码和工具使用上达到 GPT-5.2 xHigh 和 Gemini 3.0 Pro 水平。</p><ul class="paper-insights"><li>问题：Step 3.5 Flash 是一个 196B 参数、11B 活跃参数的稀疏 MoE 模型，通过可扩展的强化学习框架在数学、代码和工具使用上达到 GPT-5.2 xHigh 和 Gemini 3.0 Pro 水平。</li><li>方法：稀疏 MoE 架构：196B 总参数中仅 11B 活跃参数，大幅降低推理成本；3:1 滑动窗口/全注意力交错：优化多轮代理交互的延迟</li><li>结果：IMO-AnswerBench: 85.4%；LiveCodeBench-v6 (2024.08-2025.05): 86.4%</li><li>意义：稀疏 MoE + RL 自我提升的组合路线被验证有效；11B 活跃参数即可达到前沿水平，重新定义效率前沿</li></ul><div class="tags"><span class="tag">MoE</span><span class="tag">Agent</span><span class="tag">RL</span><span class="tag">Scaling</span><span class="tag">Reasoning</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.10604" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 09</div><h3 class="paper-title">PhyCritic: Multimodal Critic Models for Physical AI</h3></header><p class="paper-authors">Tianyi Xiong, Shihao Wang, Guilin Liu, Yi Dong, Ming Li, Heng Huang, Jan Kautz, Zhiding Yu</p><p class="paper-brief">PhyCritic 是一个专为物理 AI 任务优化的多模态评判模型，通过两阶段 RLVR 管线在物理感知、因果推理和规划任务上显著超越开源基线。</p><ul class="paper-insights"><li>问题：PhyCritic 是一个专为物理 AI 任务优化的多模态评判模型，通过两阶段 RLVR 管线在物理感知、因果推理和规划任务上显著超越开源基线。</li><li>方法：两阶段 RLVR 管线：物理技能预热 + 自参考评判微调；自参考评判机制：评判前先生成内部参考预测，再判断候选响应，提升判断稳定性</li><li>结果：在物理和通用多模态评判基准上均取得显著性能提升；超越开源基线模型</li><li>意义：RLVR 在评判模型训练中被证明非常有效；自参考机制是一个通用的评判质量提升策略</li></ul><div class="tags"><span class="tag">Physical AI</span><span class="tag">Critic Model</span><span class="tag">RLVR</span><span class="tag">Multimodal</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.11124" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article><article class="paper-card" aria-label="paper-entry"><header class="paper-header"><div class="paper-rank">Paper 10</div><h3 class="paper-title">GENIUS: Generative Fluid Intelligence Evaluation Suite</h3></header><p class="paper-authors">Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen, Haodong Li, Renrui Zhang, Xinyu Wei, Guopeng Li, Wenshan Wu, Wentao Zhang</p><p class="paper-brief">GENIUS 提出了一个评估统一多模态模型（UMM）&quot;生成式流体智力&quot;的基准，揭示现有模型在模式归纳、约束执行和上下文适应方面的显著不足。</p><ul class="paper-insights"><li>问题：GENIUS 提出了一个评估统一多模态模型（UMM）&quot;生成式流体智力&quot;的基准，揭示现有模型在模式归纳、约束执行和上下文适应方面的显著不足。</li><li>方法：生成式流体智力（GFI）概念：区别于依赖已有知识的&quot;结晶智力&quot;，关注模型在新场景中即时推理和适应的能力；三原语形式化：归纳隐式模式、执行临时约束、适应上下文知识</li><li>结果：12 个代表性模型在 GFI 任务上表现显著不足；诊断分析表明缺陷源于有限的上下文理解能力，而非生成能力不足</li><li>意义：现有 VLM 评估过度关注&quot;记忆&quot;能力，忽视了即时推理；模型的生成能力并不差，瓶颈在于上下文理解</li></ul><div class="tags"><span class="tag">Benchmark</span><span class="tag">Multimodal</span><span class="tag">Visual Generation</span><span class="tag">Evaluation</span></div><p class="paper-link"><a href="https://huggingface.co/papers/2602.11144" target="_blank" rel="noopener">Open on Hugging Face Papers</a></p></article>
      </section>

      <aside class="sidebar">
        <section>
          <h2 class="section-title">Focus Area</h2>
          <article class="focus-card"><h3 class="focus-title">Agent RL / Scaling RL Deep Dive</h3><p>以下论文与 Agent RL / Scaling RL 相关，适合优先精读并跟踪可复现价值。</p><ul><li><strong>TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions</strong>: 提出&quot;全方位密集字幕&quot;任务和 TimeChat-Captioner-7B 模型，通过六维结构化描述生成电影剧本式的视频叙事，超越 Gemini-2.5-Pro。</li><li><strong>iGRPO: Self-Feedback-Driven LLM Reasoning</strong>: iGRPO 是 GRPO 的两阶段迭代扩展，通过模型生成的草稿进行动态自条件化，在 AIME24/25 上达到新 SOTA（85.62%/79.64%）。</li><li><strong>When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning</strong>: GRU-Mem 通过引入更新门和退出门，解决了 MemAgent 在长上下文推理中记忆爆炸和不必要计算的问题，实现最高 400% 的推理加速。</li></ul></article>
        </section>

        <section>
          <h2 class="section-title">Takeaways</h2>
          <article class="takeaway"><ul><li>今日共筛选 <strong>10</strong> 篇论文，建议先读 TOP 3 获取全局脉络。</li><li>高频主题：<strong>Agent, Multimodal, RL</strong>。</li><li>第一优先论文：<strong>LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs</strong>。</li></ul></article>
        </section>
      </aside>
    </div>

    <footer>
      <span>AI Daily archive</span>
      <a href="../../index.html">Return to Archive</a>
    </footer>
  </main>

  <script>
    const cards = document.querySelectorAll('.paper-card, .focus-card, .takeaway');
    cards.forEach((card, idx) => {
      card.style.animationDelay = `${Math.min(idx * 45, 420)}ms`;
    });
  </script>
</body>
</html>
