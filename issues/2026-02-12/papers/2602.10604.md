---
title: "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters"
authors: "Ailin Huang, Ang Li, Aobo Kong, Bin Wang, Binxing Jiao 等 (200+ 位作者)"
url: "https://huggingface.co/papers/2602.10604"
upvotes: 137
tags: ["MoE", "Agent", "RL", "Scaling", "Reasoning"]
focus_area: true
---

# Step 3.5 Flash: 11B 活跃参数实现前沿级智能

## 一句话总结
Step 3.5 Flash 是一个 196B 参数、11B 活跃参数的稀疏 MoE 模型，通过可扩展的强化学习框架在数学、代码和工具使用上达到 GPT-5.2 xHigh 和 Gemini 3.0 Pro 水平。

## 关键创新
- **稀疏 MoE 架构**：196B 总参数中仅 11B 活跃参数，大幅降低推理成本
- **3:1 滑动窗口/全注意力交错**：优化多轮代理交互的延迟
- **Multi-Token Prediction (MTP-3)**：一次预测 3 个 token，加速生成
- **可扩展 RL 框架**：结合可验证信号与偏好反馈，在大规模 off-policy 训练下保持稳定

## 方法概述
模型采用稀疏 Mixture-of-Experts 架构，以 11B 活跃参数实现高效推理。训练策略上，设计了一个结合验证信号（verifiable signals）和偏好反馈（preference feedback）的可扩展强化学习框架，在数学、代码和工具使用等任务上持续自我提升。推理优化方面，采用交错的 3:1 滑动窗口/全注意力机制和 MTP-3 多 token 预测，降低多轮代理交互的延迟和成本。

## 主要结果
- IMO-AnswerBench: **85.4%**
- LiveCodeBench-v6 (2024.08-2025.05): **86.4%**
- tau2-Bench: **88.2%**
- BrowseComp (带上下文管理): **69.0%**
- Terminal-Bench 2.0: **51.0%**
- 性能与 GPT-5.2 xHigh 和 Gemini 3.0 Pro 等前沿模型相当

## 复现指南
- 模型为开源发布，可从 HuggingFace 获取
- 需要支持 MoE 推理的框架（如 vLLM、SGLang 等）
- 推荐使用多 GPU 部署（196B 总参数需要足够显存）
- MTP-3 推理需要特殊的解码策略支持

## 要点总结
1. 稀疏 MoE + RL 自我提升的组合路线被验证有效
2. 11B 活跃参数即可达到前沿水平，重新定义效率前沿
3. 代理（Agent）能力是核心设计目标，不仅仅是基准测试
4. 为工业级实际部署复杂代理提供了高密度基础