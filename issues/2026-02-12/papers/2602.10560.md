---
title: "When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning"
authors: "Leheng Sheng, Yongtao Zhang, Wenchang Ma, Yaorui Shi, Ting Huang, Xiang Wang, An Zhang, Ke Shen, Tat-Seng Chua"
url: "https://huggingface.co/papers/2602.10560"
upvotes: 21
tags: ["Long-Context", "Memory", "RL", "Reasoning"]
focus_area: true
---

# GRU-Mem: 门控循环记忆实现长上下文推理

## 一句话总结
GRU-Mem 通过引入更新门和退出门，解决了 MemAgent 在长上下文推理中记忆爆炸和不必要计算的问题，实现最高 400% 的推理加速。

## 关键创新
- **文本控制的双门机制**：更新门决定何时更新记忆，退出门决定何时停止循环
- **端到端 RL 奖励设计**：引入 r^{update} 和 r^{exit} 两个奖励信号，训练模型学会正确的更新和退出行为
- **解决 MemAgent 的两个关键缺陷**：记忆无差别更新导致爆炸、缺乏退出机制导致多余计算

## 方法概述
GRU-Mem 在 MemAgent 的 RNN 式循环处理框架上增加了两个文本控制的门。更新门（update gate）控制记忆是否需要更新——当遇到无证据的上下文块时跳过更新，避免记忆爆炸。退出门（exit gate）在收集到足够证据后立即终止循环，避免不必要的计算。通过端到端强化学习，分别用 r^{update} 和 r^{exit} 奖励信号训练这两种正确行为。

## 主要结果
- 在多种长上下文推理任务上普遍优于原始 MemAgent
- 最高实现 **400%** 推理速度加速
- 在保持甚至提升准确性的同时大幅减少计算量

## 复现指南
- 基于 MemAgent 框架扩展
- 需要实现双门机制和对应的 RL 训练
- 奖励信号设计是关键：需要标注何时应更新记忆、何时应停止

## 要点总结
1. 长上下文推理的关键不仅是"记什么"，还有"何时记"和"何时停"
2. GRU 风格的门控机制在文本记忆管理中非常有效
3. RL 奖励设计可以教会模型高效的记忆管理策略
4. 400% 加速说明大量长上下文推理存在不必要的计算浪费