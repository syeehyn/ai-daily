---
title: "iGRPO: Self-Feedback-Driven LLM Reasoning"
authors: "Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz"
url: "https://huggingface.co/papers/2602.09000"
upvotes: 13
tags: ["RL", "GRPO", "Reasoning", "Math", "Self-Improvement"]
focus_area: true
---

# iGRPO: 自反馈驱动的 LLM 推理

## 一句话总结
iGRPO 是 GRPO 的两阶段迭代扩展，通过模型生成的草稿进行动态自条件化，在 AIME24/25 上达到新 SOTA（85.62%/79.64%）。

## 关键创新
- **两阶段迭代 GRPO**：在标准 GRPO 基础上增加自条件化精炼阶段
- **动态自条件化**：Stage 1 采样多个探索性草稿并选择最佳；Stage 2 将最佳草稿附加到提示中进行精炼
- **延迟熵崩塌**：精炼包装器改变了学习动态，延缓了策略的过早收敛

## 方法概述
iGRPO 分两阶段工作。Stage 1：采样多个探索性草稿，使用与优化相同的标量奖励信号选择最高奖励的草稿。Stage 2：将最佳草稿附加到原始提示，对草稿条件化的精炼进行 GRPO 风格更新，训练策略超越其最强先前尝试。在匹配的 rollout 预算下，iGRPO 持续优于标准 GRPO。

## 主要结果
- 应用于 OpenReasoning-Nemotron-7B + AceReason-Math：
  - AIME24: **85.62%** (新 SOTA)
  - AIME25: **79.64%** (新 SOTA)
- 在 Nemotron-H-8B-Base 和 DeepSeek-R1 Distilled 上均持续优于 GRPO
- 精炼包装器泛化超越 GRPO 变体，受益于生成式评判

## 复现指南
- 基于 GRPO 框架扩展，需要实现两阶段训练
- 核心修改在于 Stage 2 的草稿条件化
- 在 AIME 等数学推理基准上评估
- 需要足够的 rollout 预算

## 要点总结
1. 自反馈是提升 RL 推理训练效果的有效策略
2. "先探索后精炼"范式优于直接优化
3. 延迟熵崩塌是一个重要的学习动态改善
4. 在 7B 规模模型上即可达到数学推理 SOTA